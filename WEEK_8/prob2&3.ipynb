{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import poisson\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Poisson(object):\n",
    "    cache_pmf = {}\n",
    "    cache_sf = {}\n",
    "    cache = {}\n",
    "    MAX_CUTOFF = 25\n",
    "\n",
    "    @classmethod\n",
    "    def pmf_series(cls, mu, cutoff):\n",
    "        assert isinstance(mu, int), \"mu should be an integer.\"\n",
    "        assert isinstance(cutoff, int), \"cutoff should be an integer\"\n",
    "\n",
    "        if (mu, cutoff) not in cls.cache:\n",
    "            cls._calculate_pmf_series(mu, cutoff)\n",
    "\n",
    "        return cls.cache[(mu, cutoff)]\n",
    "\n",
    "    @classmethod\n",
    "    def _calculate_pmf_series(cls, mu, cutoff):\n",
    "\n",
    "        if mu not in cls.cache_pmf:\n",
    "            print(\"Calculate poisson ...\")\n",
    "            cls.cache_pmf[mu] = poisson.pmf(np.arange(cls.MAX_CUTOFF + 1), mu)\n",
    "            cls.cache_sf[mu] = poisson.sf(np.arange(cls.MAX_CUTOFF + 1), mu)\n",
    "\n",
    "        out = np.copy(cls.cache_pmf[mu][:cutoff+1])\n",
    "        out[-1] += cls.cache_sf[mu][cutoff]\n",
    "\n",
    "        cls.cache[(mu, cutoff)] = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code begins by defining key problem parameters, such as the maximum number of bikes at each location, the maximum number that can be transferred between locations, the rental reward per bike, the cost of moving bikes, and the discount factor.\n",
    "\n",
    "Next, the value function is initialized to zero, and the value iteration algorithm is executed until it converges, based on a specified tolerance level.\n",
    "\n",
    "For each state \n",
    "(\n",
    "i\n",
    ",\n",
    "j\n",
    ")\n",
    "(i,j) and all possible actions, the Q-value is computed using the transition probabilities, reward function, and discount factor. The action with the highest Q-value is selected for each state, and the value function is updated accordingly. This process repeats until the value function converges to its optimal state.\n",
    "\n",
    "Finally, the optimal value function is produced. It's important to note that the code assumes the Poisson distribution and probability mass function are precomputed outside the iteration loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIterationSolver(object):\n",
    "\n",
    "    capacity = 20\n",
    "    rental_reward = 10.\n",
    "    moving_cost = 2.\n",
    "    max_moving = 5\n",
    "\n",
    "    # bad_action_cost should always non-negative.\n",
    "    # when bad_action_cost == 0, bad action is not punished, otherwise bad action is published according to value\n",
    "    # set here.\n",
    "    # bad_action_cost = 0\n",
    "    bad_action_cost = 100.\n",
    "\n",
    "    request_mean_G1 = 3\n",
    "    request_mean_G2 = 4\n",
    "    return_mean_G1 = 3\n",
    "    return_mean_G2 = 2\n",
    "\n",
    "    discount = 0.9\n",
    "\n",
    "    PolicyEvaluationError = 0.01\n",
    "\n",
    "    policy = None\n",
    "    value = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.policy = np.zeros([self.capacity + 1]*2, int)\n",
    "        self.value = np.zeros([self.capacity + 1]*2)\n",
    "\n",
    "        self._reward1 = self.expected_rental_reward(self.request_mean_G1)\n",
    "        self._reward2 = self.expected_rental_reward(self.request_mean_G2)\n",
    "\n",
    "        assert self.bad_action_cost >= 0\n",
    "\n",
    "    def bellman(self, action, s1, s2):\n",
    "        transp1 = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1, -action)\n",
    "        transp2 = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2, action)\n",
    "        transp = np.outer(transp1, transp2)\n",
    "\n",
    "        return self._reward1[s1] + self._reward2[s2] - self.expected_moving_cost(s1, s2, action) + \\\n",
    "               self.discount * sum((transp * self.value).flat)\n",
    "               # policy evaluation\n",
    "    def policy_evaluation(self):\n",
    "        ''' Keep pocliy fixed and update value. '''\n",
    "        while True:\n",
    "            diff = 0.\n",
    "            it = np.nditer([self.policy], flags=['multi_index'])\n",
    "\n",
    "            while not it.finished:\n",
    "                action = it[0]\n",
    "                s1, s2 = it.multi_index\n",
    "\n",
    "                _temp = self.value[s1, s2]\n",
    "\n",
    "                self.value[s1, s2] = self.bellman(action=action, s1=s1, s2=s2)\n",
    "\n",
    "                diff = max(diff, abs(self.value[s1, s2] - _temp))\n",
    "\n",
    "                it.iternext()\n",
    "\n",
    "            print(diff)\n",
    "            if diff < self.PolicyEvaluationError:\n",
    "                break\n",
    "\n",
    "    def policy_update(self):\n",
    "        is_policy_changed = False\n",
    "\n",
    "        it = np.nditer([self.policy], flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            s1, s2 = it.multi_index\n",
    "\n",
    "            _max_val = -1\n",
    "            _pol = None\n",
    "\n",
    "            for act in range(-self.max_moving, self.max_moving + 1):\n",
    "                _val = self.bellman(action=act, s1=s1, s2=s2)\n",
    "                if _val > _max_val:\n",
    "                    _max_val = _val\n",
    "                    _pol = act\n",
    "\n",
    "            if self.policy[s1, s2] != _pol:\n",
    "                is_policy_changed = True\n",
    "                self.policy[s1, s2] = _pol\n",
    "\n",
    "            it.iternext()\n",
    "            return is_policy_changed\n",
    "\n",
    "    def expected_moving_cost(self, s1, s2, action):\n",
    "        if action == 0:\n",
    "            return 0.\n",
    "\n",
    "        # moving from state s1 into state s2\n",
    "        if action > 0:\n",
    "            p = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1)\n",
    "            cost = self._gen_move_cost_array(action)\n",
    "            return cost.dot(p)\n",
    "\n",
    "        # moving from state s2 into state s1\n",
    "        p = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2)\n",
    "        cost = self._gen_move_cost_array(action)\n",
    "        return cost.dot(p)\n",
    "\n",
    "    def _gen_move_cost_array(self, action):\n",
    "        '''\n",
    "        Generate an array based on which costs of move is calculated.\n",
    "        \n",
    "        If action > available GBikes, then this action is considered as a bad action.\n",
    "        \n",
    "        when self.bad_move_cost == 0, bad action is not punished. The system will move max possible GBikes.\n",
    "        \n",
    "        When self.bad_move_cost >0, bad action is punished indicated by this variable.\n",
    "        \n",
    "        :param action: Number of GBikes that will be moved from station 1 to station 2.\n",
    "        :return: \n",
    "        '''\n",
    "        _action = abs(action)\n",
    "\n",
    "        # Don't punish bad action:\n",
    "        if self.bad_action_cost == 0:\n",
    "            cost = np.asarray(\n",
    "                [ii if ii < _action else _action for ii in range(self.capacity+1)]\n",
    "            ) * self.moving_cost\n",
    "\n",
    "        # bad action is punished\n",
    "        else:\n",
    "            cost = np.asarray(\n",
    "                [self.bad_action_cost if ii < _action else _action for ii in range(self.capacity + 1)]\n",
    "            ) * self.moving_cost\n",
    "        return cost\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def expected_rental_reward(cls, expected_request):\n",
    "        return np.asarray([cls._state_reward(s, expected_request) for s in range(cls.capacity + 1)])\n",
    "\n",
    "    @classmethod\n",
    "    def _state_reward(cls, s, mu):\n",
    "        rewards = cls.rental_reward * np.arange(s + 1)\n",
    "        p = Poisson.pmf_series(mu, cutoff=s)\n",
    "        return rewards.dot(p)\n",
    "\n",
    "    def transition_probabilty(self, s, req, ret, action=0):\n",
    "        '''    \n",
    "        :param s: Current State\n",
    "        :param req: Mean value of requests\n",
    "        :param ret: Mean value of returns\n",
    "        :param action: Action. Positive means move in. Negative means move out.\n",
    "        :return: Transition probability.\n",
    "        '''\n",
    "\n",
    "        _ret_sz = self.max_moving + self.capacity\n",
    "\n",
    "        p_req = Poisson.pmf_series(req, s)\n",
    "        p_ret = Poisson.pmf_series(ret, _ret_sz)\n",
    "        p = np.outer(p_req, p_ret)\n",
    "\n",
    "        transp = np.asarray([p.trace(offset) for offset in range(-s, _ret_sz + 1)])\n",
    "\n",
    "        assert abs(action) <= self.max_moving, \"action can be large than %s.\" % self.max_moving\n",
    "\n",
    "        # No GBikes are being moved\n",
    "        if action == 0:\n",
    "            transp[20] += sum(transp[21:])\n",
    "            return transp[:21]\n",
    "\n",
    "        # Move GBikes from station 1 to station 2\n",
    "        if action > 0:\n",
    "            transp[self.capacity-action] += sum(transp[self.capacity-action+1:])\n",
    "            transp[self.capacity-action+1:] = 0\n",
    "\n",
    "            return np.roll(transp, shift=action)[:self.capacity+1]\n",
    "\n",
    "        # Move GBikes from station 2 to station 1\n",
    "        action = -action\n",
    "        transp[action] += sum(transp[:action])\n",
    "        transp[:action] = 0\n",
    "        transp[action+self.capacity] += sum(transp[action+self.capacity+1:])\n",
    "        transp[action+self.capacity+1:] = 0\n",
    "\n",
    "        return np.roll(transp, shift=-action)[:self.capacity+1]\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        '''\n",
    "        Caveat: the situation where the policy continually switches between two or more policies that are equally good is not considered yet. \n",
    "        :return: \n",
    "        '''\n",
    "        self.policy_evaluation()\n",
    "        while self.policy_update():\n",
    "            self.policy_evaluation()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "solver = PolicyIterationSolver()\n",
    "\n",
    "for ii in range(4):\n",
    "    solver.policy_evaluation()\n",
    "    solver.policy_update()\n",
    "\n",
    "print(solver.policy)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "CS = plt.contour(solver.policy, levels=range(-6, 6))\n",
    "plt.clabel(CS)\n",
    "plt.xlim([0, 20])\n",
    "plt.ylim([0, 20])\n",
    "plt.axis('equal')\n",
    "plt.xticks(range(21))\n",
    "plt.yticks(range(21))\n",
    "plt.grid('on')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.pcolor(solver.value)\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we start by defining key parameters for the problem, such as the maximum number of bikes allowed at each location, the maximum number of bikes that can be transferred between locations, the rental reward per bike, the cost of moving bikes, the extra parking fee for storing more than 10 bikes, and the discount factor.\n",
    "\n",
    "Next, we initialize the value function to zero and execute the value iteration algorithm until it converges, based on a specified tolerance level.\n",
    "\n",
    "For each state \n",
    "(\n",
    "i\n",
    ",\n",
    "j\n",
    ")\n",
    "(i,j) and every possible action, the code computes the corresponding Q-value using an adjusted reward function that incorporates both the additional parking cost and the free bike movement offered by the employee. The action with the highest Q-value is then chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIterationSolver(object):\n",
    "\n",
    "    capacity = 20\n",
    "    rental_reward = 10.\n",
    "    moving_cost = 2.\n",
    "    max_moving = 5\n",
    "    parking_cost = 4.\n",
    "\n",
    "    # bad_action_cost should always non-negative.\n",
    "    # when bad_action_cost == 0, bad action is not punished, otherwise bad action is published according to value\n",
    "    # set here.\n",
    "    # bad_action_cost = 0\n",
    "    bad_action_cost = 100.\n",
    "\n",
    "    request_mean_G1 = 3\n",
    "    request_mean_G2 = 4\n",
    "    return_mean_G1 = 3\n",
    "    return_mean_G2 = 2\n",
    "\n",
    "    discount = 0.9\n",
    "\n",
    "    PolicyEvaluationError = 0.01\n",
    "\n",
    "    policy = None\n",
    "    value = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.policy = np.zeros([self.capacity + 1]*2, int)\n",
    "        self.value = np.zeros([self.capacity + 1]*2)\n",
    "\n",
    "        self._reward1 = self.expected_rental_reward(self.request_mean_G1)\n",
    "        self._reward2 = self.expected_rental_reward(self.request_mean_G2)\n",
    "\n",
    "        assert self.bad_action_cost >= 0\n",
    "\n",
    "    def bellman(self, action, s1, s2):\n",
    "        transp1 = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1, -action)\n",
    "        transp2 = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2, action)\n",
    "        transp = np.outer(transp1, transp2)\n",
    "\n",
    "        return self._reward1[s1] + self._reward2[s2] - self.expected_moving_cost(s1, s2, action) + \\\n",
    "               self.discount * sum((transp * self.value).flat)\n",
    "\n",
    "    # policy evaluation\n",
    "    def policy_evaluation(self):\n",
    "    ''' Keep pocliy fixed and update value. '''\n",
    "        while True:\n",
    "            diff = 0.\n",
    "            it = np.nditer([self.policy], flags=['multi_index'])\n",
    "\n",
    "            while not it.finished:\n",
    "                action = it[0]\n",
    "                s1, s2 = it.multi_index\n",
    "\n",
    "                _temp = self.value[s1, s2]\n",
    "\n",
    "                self.value[s1, s2] = self.bellman(action=action, s1=s1, s2=s2)\n",
    "\n",
    "                diff = max(diff, abs(self.value[s1, s2] - _temp))\n",
    "\n",
    "                it.iternext()\n",
    "\n",
    "            print(diff)\n",
    "            if diff < self.PolicyEvaluationError:\n",
    "                break\n",
    "\n",
    "    def policy_update(self):\n",
    "        is_policy_changed = False\n",
    "\n",
    "        it = np.nditer([self.policy], flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            s1, s2 = it.multi_index\n",
    "\n",
    "            _max_val = -1\n",
    "            _pol = None\n",
    "\n",
    "            for act in range(-self.max_moving, self.max_moving + 1):\n",
    "                _val = self.bellman(action=act, s1=s1, s2=s2)\n",
    "                if _val > _max_val:\n",
    "                    _max_val = _val\n",
    "                    _pol = act\n",
    "\n",
    "            if self.policy[s1, s2] != _pol:\n",
    "                is_policy_changed = True\n",
    "                self.policy[s1, s2] = _pol\n",
    "it.iternext()\n",
    "\n",
    "        return is_policy_changed\n",
    "\n",
    "    def expected_moving_cost(self, s1, s2, action):\n",
    "        if action == 0:\n",
    "            return 0.\n",
    "\n",
    "        # moving from state s1 into state s2\n",
    "        if action > 0:\n",
    "            p = self.transition_probabilty(s1, self.request_mean_G1, self.return_mean_G1)\n",
    "            cost = self._gen_move_cost_array(action)\n",
    "            if action > 10:\n",
    "              cost += self.parking_cost * (action - 10)\n",
    "            return cost.dot(p)\n",
    "\n",
    "        # moving from state s2 into state s1\n",
    "        p = self.transition_probabilty(s2, self.request_mean_G2, self.return_mean_G2)\n",
    "        cost = self._gen_move_cost_array(action)\n",
    "        if action > 10:\n",
    "            cost += self.parking_cost * (action - 10)\n",
    "        return cost.dot(p)\n",
    "\n",
    "    def _gen_move_cost_array(self, action):\n",
    "        '''\n",
    "        Generate an array based on which costs of move is calculated.\n",
    "        \n",
    "        If action > available GBikes, then this action is considered as a bad action.\n",
    "        \n",
    "        when self.bad_move_cost == 0, bad action is not punished. The system will move max possible GBikes.\n",
    "        \n",
    "        When self.bad_move_cost >0, bad action is punished indicated by this variable.\n",
    "        \n",
    "        :param action: Number of GBikes that will be moved from station 1 to station 2.\n",
    "        :return: \n",
    "        '''\n",
    "        _action = abs(action)\n",
    "\n",
    "        if _action != 0:\n",
    "          _action -= 1\n",
    "\n",
    "        # Don't punish bad action:\n",
    "        if self.bad_action_cost == 0:\n",
    "            cost = np.asarray(\n",
    "                [ii if ii < _action else _action for ii in range(self.capacity+1)]\n",
    "            ) * self.moving_cost\n",
    "\n",
    "        # bad action is punished\n",
    "        else:\n",
    "            cost = np.asarray(\n",
    "                [self.bad_action_cost if ii < _action else _action for ii in range(self.capacity + 1)]\n",
    "            ) * self.moving_cost\n",
    "        return cost\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def expected_rental_reward(cls, expected_request):\n",
    "        return np.asarray([cls._state_reward(s, expected_request) for s in range(cls.capacity + 1)])\n",
    "\n",
    "    @classmethod\n",
    "    def _state_reward(cls, s, mu):\n",
    "        rewards = cls.rental_reward * np.arange(s + 1)\n",
    "        p = Poisson.pmf_series(mu, cutoff=s)\n",
    "        return rewards.dot(p)\n",
    "\n",
    "    def transition_probabilty(self, s, req, ret, action=0):\n",
    "        '''    \n",
    "        :param s: Current State\n",
    "        :param req: Mean value of requests\n",
    "        :param ret: Mean value of returns\n",
    "        :param action: Action. Positive means move in. Negative means move out.\n",
    "        :return: Transition probability.\n",
    "        '''\n",
    "\n",
    "        _ret_sz = self.max_moving + self.capacity\n",
    "\n",
    "        p_req = Poisson.pmf_series(req, s)\n",
    "        p_ret = Poisson.pmf_series(ret, _ret_sz)\n",
    "        p = np.outer(p_req, p_ret)\n",
    "\n",
    "        transp = np.asarray([p.trace(offset) for offset in range(-s, _ret_sz + 1)])\n",
    "\n",
    "        assert abs(action) <= self.max_moving, \"action can be large than %s.\" % self.max_moving\n",
    "\n",
    "        # No GBikes are being moved\n",
    "        if action == 0:\n",
    "            transp[20] += sum(transp[21:])\n",
    "            return transp[:21]\n",
    "            # Move GBikes from station 1 to station 2\n",
    "        if action > 0:\n",
    "            transp[self.capacity-action] += sum(transp[self.capacity-action+1:])\n",
    "            transp[self.capacity-action+1:] = 0\n",
    "\n",
    "            return np.roll(transp, shift=action)[:self.capacity+1]\n",
    "\n",
    "        # Move GBikes from station 2 to station 1\n",
    "        action = -action\n",
    "        transp[action] += sum(transp[:action])\n",
    "        transp[:action] = 0\n",
    "\n",
    "        transp[action+self.capacity] += sum(transp[action+self.capacity+1:])\n",
    "        transp[action+self.capacity+1:] = 0\n",
    "\n",
    "        return np.roll(transp, shift=-action)[:self.capacity+1]\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        '''\n",
    "        Caveat: the situation where the policy continually switches between two or more policies that are equally good is not considered yet. \n",
    "        :return: \n",
    "        '''\n",
    "        self.policy_evaluation()\n",
    "        while self.policy_update():\n",
    "            self.policy_evaluation()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "solver = PolicyIterationSolver()\n",
    "\n",
    "for ii in range(4):\n",
    "    solver.policy_evaluation()\n",
    "    solver.policy_update()\n",
    "\n",
    "print(solver.policy)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "CS = plt.contour(solver.policy, levels=range(-6, 6))\n",
    "plt.clabel(CS)\n",
    "plt.xlim([0, 20])\n",
    "plt.ylim([0, 20])\n",
    "plt.axis('equal')\n",
    "plt.xticks(range(21))\n",
    "plt.yticks(range(21))\n",
    "plt.grid('on')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.pcolor(solver.value)\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
